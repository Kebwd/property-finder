# GitHub Actions Workflow for Daily Property Scraping
# This runs in the cloud 24/7, no PC required!

name: Daily Property Scraper

on:
  schedule:
    # Run at 2:00 AM UTC daily (10:00 AM Hong Kong time)
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape-properties:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client
    
    - name: Install Python dependencies
      run: |
        cd scraper
        echo "Checking requirements.txt content:"
        cat requirements.txt
        pip install --upgrade pip
        pip cache purge
        pip install -r requirements.txt
        # Ensure scrapy-selenium is installed
        pip install scrapy-selenium>=1.7.0
        echo "✅ All dependencies installed"
        
    - name: Verify dependencies
      run: |
        cd scraper
        python -c "from scrapy_selenium import SeleniumRequest; print('✅ scrapy-selenium ready')"
        python -m scrapy list
        echo "✅ All spiders loading correctly"
    
    - name: Create output directory
      run: |
        cd scraper
        mkdir -p daily_output/$(date +%Y-%m-%d)
    
    - name: Run daily scraper
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        SCRAPER_MODE: daily
        LOG_LEVEL: INFO
      run: |
        cd scraper
        echo "Starting daily property scraper..."
        python daily_scraper.py
        echo "Scraper completed successfully!"
    
    - name: Upload logs and results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: scraper-logs-${{ github.run_number }}
        path: |
          scraper/daily_output/
          scraper/*.log
        retention-days: 30
    
    - name: Notify on failure
      if: failure()
      run: |
        echo "Scraper failed! Check the logs for details."
        exit 1
