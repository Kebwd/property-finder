# GitHub Actions Workflow for Daily Property Scraping
# This runs in the cloud 24/7, no PC required!

name: Daily Property Scraper

on:
  schedule:
    # Run at 2:00 AM UTC daily (10:00 AM Hong Kong time)
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape-properties:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client
        # Install Chrome and dependencies for selenium
        wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        # Install additional dependencies
        sudo apt-get install -y xvfb
    
    - name: Install Python dependencies
      run: |
        cd scraper
        echo "Checking requirements.txt content:"
        cat requirements.txt
        pip install --upgrade pip
        pip cache purge
        pip install -r requirements.txt
        # Ensure additional dependencies are installed
        pip install beautifulsoup4>=4.12.0 lxml>=4.9.0 aiohttp>=3.8.0
        # Ensure scrapy-selenium is installed with correct version
        pip install scrapy-selenium>=0.0.7
        echo "✅ All dependencies installed"
        
    - name: Verify dependencies
      run: |
        cd scraper
        echo "Running comprehensive dependency check..."
        python check_dependencies.py
        echo "✅ All dependencies verified"
    
    - name: Create output directory
      run: |
        cd scraper
        mkdir -p daily_output/$(date +%Y-%m-%d)
    
    - name: Run daily scraper
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        SCRAPER_MODE: daily
        LOG_LEVEL: INFO
        # Chrome/Selenium settings for CI
        DISPLAY: ":99"
        CHROME_OPTIONS: "--headless --no-sandbox --disable-dev-shm-usage --disable-gpu"
      run: |
        cd scraper
  # Start virtual display for Chrome
  Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &
  echo "Starting daily property scraper (house then store)..."
  # Run house spider (default) and continue even if it fails
  python daily_scraper.py --houses daily || echo "house spider failed but continuing"
  # Run store spider as well; allow failure without aborting the job
  python daily_scraper.py --stores daily || echo "store spider failed"
  echo "Scraper runs completed (one or both may have failed)"
    
    - name: Upload logs and results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: scraper-logs-${{ github.run_number }}
        path: |
          scraper/daily_output/
          scraper/*.log
        retention-days: 30
    
    - name: Notify on failure
      if: failure()
      run: |
        echo "Scraper failed! Check the logs for details."
        exit 1
