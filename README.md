# ğŸ  Property Finder - Cloud Native

A modern property finder application with automated daily scraping, built for Vercel + Supabase deployment.

## ğŸŒŸ Features

- **ğŸ” Property Search**: Search houses and businesses with advanced filters
- **ğŸ“Š Data Visualization**: Interactive maps and charts
- **ğŸ•·ï¸ Automated Scraping**: Daily property data collection via GitHub Actions  
- **ğŸ“ CSV Import/Export**: Bulk data management
- **ğŸŒ RESTful API**: Serverless API functions
- **ğŸ“± Modern UI**: Responsive React frontend
- **â˜ï¸ Cloud-Native**: Full serverless deployment with Vercel + Supabase

## ğŸš€ Quick Deploy to Vercel + Supabase

### 1. Setup Supabase Database
- Go to [supabase.com](https://supabase.com) and create a new project
- Go to SQL Editor and run the migration scripts from `property-finder-api/migrations/`
- Get your Project URL and Service Role Key from Settings â†’ API

### 2. Deploy to Vercel
- Fork this repository to your GitHub
- Go to [vercel.com](https://vercel.com) and import your fork
- Set environment variables:
  - `SUPABASE_URL`: Your Supabase project URL
  - `SUPABASE_SERVICE_ROLE_KEY`: Your Supabase service role key

### 3. Configure GitHub Actions (Optional)

For automated scraping, set repository secrets:
- `VERCEL_DEPLOY_URL`: Your deployed Vercel app URL
- `SCRAPER_API_KEY`: Generate a secure API key for scraper authentication

## ğŸ› ï¸ Environment Variables

### Required for Vercel Deployment:
```bash
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key
```

### For Local Development:
```bash
DATABASE_URL=postgresql://postgres:password@localhost:5432/property_finder
API_BASE_URL=http://localhost:5000
VITE_API_URL=http://localhost:5000
GEOCODING_API_KEY=your-google-maps-api-key
```

## ğŸ“ Project Structure

```
property-finder/
â”œâ”€â”€ api/                    # Vercel serverless functions
â”‚   â”œâ”€â”€ health.js          # Health check endpoint
â”‚   â”œâ”€â”€ debug.js           # Debug information
â”‚   â””â”€â”€ search.js          # Property search API
â”œâ”€â”€ src/                   # React frontend
â”‚   â”œâ”€â”€ App.jsx           # Main application component
â”‚   â”œâ”€â”€ components/       # Reusable UI components
â”‚   â””â”€â”€ utils/           # Utility functions
â”œâ”€â”€ property-finder-api/   # Original API (for reference)
â”œâ”€â”€ scraper/              # Python scraping system
â”œâ”€â”€ vercel.json           # Vercel deployment config
â””â”€â”€ package.json          # Node.js dependencies
```

## ğŸ”§ API Endpoints

- **GET** `/api/health` - Health check and database status
- **GET** `/api/debug` - Debug information and table counts
- **GET** `/api/search` - Property search with filters
  - Query parameters: `address`, `type`, `page`, `limit`, `category`, `filter_date`

## ğŸ•·ï¸ Automated Scraping

- **Schedule**: Daily at 2 AM UTC via GitHub Actions
- **Trigger**: Automated workflow or manual dispatch
- **Security**: API key authentication
- **Target**: Vercel deployment endpoint

## ğŸ§ª Testing the Deployment

```bash
# Test health endpoint
curl https://your-app.vercel.app/api/health

# Test search endpoint
curl "https://your-app.vercel.app/api/search?address=é¦™æ¸¯&type=all&page=1"

# Test debug information
curl https://your-app.vercel.app/api/debug
```

## ğŸ“š Documentation

- `RAILWAY_DEPLOY.md` - Detailed deployment guide
- `DEPLOY_STEPS.md` - Step-by-step instructions  
- `SCRAPING_SETUP.md` - Scraping configuration
- `scraper/DAILY_SETUP_GUIDE.md` - Scraper details

## ğŸ”’ Security

- Environment variables for secrets
- API key authentication for scraping
- Rate limiting on endpoints
- CORS configuration

## ğŸŒ Live App

Once deployed, your app will be available at:
`https://your-app-name.railway.app`

## ğŸ¯ Next Steps

1. **Deploy**: Follow the deployment guide
2. **Configure**: Set environment variables
3. **Test**: Run verification script
4. **Monitor**: Check daily scraping automation
5. **Use**: Start adding/searching properties!

---

Built with â¤ï¸ using Node.js, React, Python, and Railway
