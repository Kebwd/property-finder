#!/usr/bin/env python3
"""
Consistent Scraping Test Suite
Tests the enhanced anti-bot system for maximum success rate
"""

import sys
import os
import subprocess
import time
import json
from datetime import datetime
from pathlib import Path

# Add the scraper directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

def test_proxy_pool():
    """Test the proxy pool functionality"""
    print("ðŸŒ Testing Proxy Pool...")
    
    try:
        from middlewares.consistent_scraping import ProxyPool
        
        pool = ProxyPool()
        stats = pool.get_proxy_stats()
        
        print(f"   ðŸ“Š Total proxies loaded: {stats['total_proxies']}")
        print(f"   âœ… Healthy proxies: {stats['healthy_proxies']}")
        print(f"   âŒ Failed proxies: {stats['failed_proxies']}")
        print(f"   ðŸ“ˆ Success rate: {stats['success_rate']:.2%}")
        
        if stats['total_proxies'] > 0:
            best_proxy = pool.get_best_proxy()
            if best_proxy:
                print(f"   ðŸ† Best proxy: {best_proxy}")
                return True
            else:
                print("   âš ï¸  No working proxies available")
                return False
        else:
            print("   â„¹ï¸  No proxies configured (will use direct connection)")
            return True
            
    except Exception as e:
        print(f"   âŒ Proxy pool test failed: {e}")
        return False

def test_consistent_manager():
    """Test the consistent scraping manager"""
    print("\\nðŸ¤– Testing Consistent Scraping Manager...")
    
    try:
        from middlewares.consistent_scraping import consistent_manager
        
        # Test with a simple request
        test_url = "https://httpbin.org/ip"
        print(f"   ðŸ”— Testing request to: {test_url}")
        
        start_time = time.time()
        response = consistent_manager.make_request(test_url)
        duration = time.time() - start_time
        
        if response and response.status_code == 200:
            print(f"   âœ… Request successful in {duration:.2f}s")
            
            # Show stats
            stats = consistent_manager.get_stats()
            print(f"   ðŸ“Š Success rate: {stats['success_rate']:.2%}")
            print(f"   â±ï¸  Current delay: {stats['current_delay']:.1f}s")
            
            return True
        else:
            print("   âŒ Request failed")
            return False
            
    except Exception as e:
        print(f"   âŒ Consistent manager test failed: {e}")
        return False

def test_lianjia_access():
    """Test actual Lianjia access with consistent protection"""
    print("\\nðŸ  Testing Lianjia Access with Consistent Protection...")
    
    try:
        from middlewares.consistent_scraping import consistent_manager
        
        test_urls = [
            'https://sh.lianjia.com/',
            'https://bj.lianjia.com/',
        ]
        
        success_count = 0
        
        for url in test_urls:
            print(f"   ðŸ“ Testing: {url}")
            
            response = consistent_manager.make_request(url)
            
            if response and response.status_code == 200:
                content_length = len(response.text)
                
                if content_length > 10000:  # Substantial content
                    if 'éªŒè¯ç ' in response.text or 'captcha' in response.text.lower():
                        print(f"   âš ï¸  CAPTCHA detected but response received - {url}")
                    else:
                        print(f"   âœ… SUCCESS - {url} (Content: {content_length:,} chars)")
                        success_count += 1
                else:
                    print(f"   âš ï¸  Short response - possible blocking - {url}")
            else:
                print(f"   âŒ Failed - {url}")
        
        success_rate = success_count / len(test_urls)
        print(f"   ðŸ“Š Overall success rate: {success_rate:.2%}")
        
        return success_rate > 0.5  # Consider 50%+ success as good
        
    except Exception as e:
        print(f"   âŒ Lianjia test failed: {e}")
        return False

def test_spider_with_consistent_protection():
    """Test spider with consistent anti-bot protection"""
    print("\\nðŸ•·ï¸ Testing Spider with Consistent Protection...")
    
    cmd = [
        sys.executable, "-m", "scrapy", "crawl", "house_spider",
        "-a", "mode=test",
        "-a", "max_pages=1",
        "-L", "INFO",
        "-s", "CLOSESPIDER_ITEMCOUNT=3",  # Stop after 3 items
        "-o", "consistent_test_output.json",
        "--logfile", "consistent_test.log"
    ]
    
    print(f"   ðŸš€ Running: {' '.join(cmd[-6:])}")  # Show last 6 args
    
    try:
        start_time = time.time()
        result = subprocess.run(
            cmd,
            cwd=Path(__file__).parent,
            capture_output=True,
            text=True,
            timeout=300  # 5 minute timeout
        )
        duration = time.time() - start_time
        
        print(f"   â±ï¸  Completed in {duration:.1f} seconds")
        print(f"   ðŸ“Š Return code: {result.returncode}")
        
        if result.returncode == 0:
            # Check output
            output_file = Path("consistent_test_output.json")
            if output_file.exists():
                try:
                    with open(output_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        item_count = len(data) if data else 0
                        
                    print(f"   ðŸ“Š Items scraped: {item_count}")
                    
                    if item_count > 0:
                        sample = data[0]
                        print(f"   ðŸ  Sample: {sample.get('estate_name_zh', 'N/A')}")
                        print(f"   âœ… Consistent protection working!")
                        
                        # Cleanup
                        output_file.unlink()
                        Path("consistent_test.log").unlink(missing_ok=True)
                        
                        return True
                    else:
                        print("   âš ï¸  No items scraped - checking logs...")
                        
                        # Show log excerpt
                        log_file = Path("consistent_test.log")
                        if log_file.exists():
                            with open(log_file, 'r', encoding='utf-8') as f:
                                log_content = f.read()
                                if 'Blocking detected' in log_content:
                                    print("   ðŸš« Blocking detected but system is working")
                                elif 'SUCCESS' in log_content:
                                    print("   âœ… Requests successful but no items extracted")
                                else:
                                    print("   â“ Check full log for details")
                        
                        return False
                        
                except json.JSONDecodeError:
                    print("   âŒ Invalid JSON output")
                    return False
            else:
                print("   âŒ No output file created")
                return False
        else:
            print(f"   âŒ Spider failed with return code: {result.returncode}")
            if result.stderr:
                print(f"   ðŸ“‹ Error: {result.stderr[-200:]}")  # Last 200 chars
            return False
            
    except subprocess.TimeoutExpired:
        print("   â° Test timed out (5+ minutes)")
        return False
    except Exception as e:
        print(f"   âŒ Test failed: {e}")
        return False

def main():
    """Run comprehensive consistent scraping tests"""
    print("ðŸš€ CONSISTENT SCRAPING TEST SUITE")
    print("=" * 60)
    print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    tests = [
        ("Proxy Pool", test_proxy_pool),
        ("Consistent Manager", test_consistent_manager),
        ("Lianjia Access", test_lianjia_access),
        ("Spider Integration", test_spider_with_consistent_protection),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"   ðŸ’¥ {test_name} crashed: {e}")
            results[test_name] = False
    
    # Summary
    print("\\n" + "=" * 60)
    print("ðŸ“Š TEST RESULTS SUMMARY")
    print("=" * 60)
    
    total_tests = len(results)
    passed_tests = sum(results.values())
    
    for test_name, passed in results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"{status}: {test_name}")
    
    print(f"\\nðŸ“ˆ Overall: {passed_tests}/{total_tests} tests passed ({passed_tests/total_tests:.1%})")
    
    if passed_tests == total_tests:
        print("\\nðŸŽ‰ ALL TESTS PASSED - Consistent scraping is ready!")
        print("\\nðŸ’¡ Next steps:")
        print("   1. Add premium proxies to proxy_list.txt for best results")
        print("   2. Run: python daily_scraper.py --houses daily")
        print("   3. Monitor success rates and adjust delays if needed")
    elif passed_tests >= total_tests * 0.75:
        print("\\nâœ… MOSTLY WORKING - Minor issues detected")
        print("\\nðŸ’¡ Recommendations:")
        print("   1. Add more proxies for better success rates")
        print("   2. Consider premium proxy services")
        print("   3. Test during off-peak hours")
    else:
        print("\\nâš ï¸  SIGNIFICANT ISSUES - Need troubleshooting")
        print("\\nðŸ”§ Troubleshooting steps:")
        print("   1. Check internet connection")
        print("   2. Verify proxy configurations")
        print("   3. Try during different time periods")
        print("   4. Consider using premium proxy services")
    
    return passed_tests >= total_tests * 0.5

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
